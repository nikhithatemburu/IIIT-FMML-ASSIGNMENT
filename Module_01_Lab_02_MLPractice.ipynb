{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhithatemburu/IIIT-FMML-ASSIGNMENT/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f361dc-6a4f-4700-a510-de71c4eb3515"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa46157-bd05-4339-e77f-451a41a5dbbf"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99315a6a-46e2-420c-e9f3-bfcf079e07ce"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697ff307-f662-4066-ed9c-4e5dce3d612d"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1bcff2-9d48-47cf-bc30-ea89054deb05"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34179fec-78a4-4c0f-b96c-ce6d38c79682"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?**\n",
        "The size of the validation set can have a significant impact on the accuracy estimation and the model's performance. Here's how it can be affected:\n",
        "\n",
        "Increasing the Percentage of the Validation Set:\n",
        "\n",
        "Increased Robustness: With a larger validation set, you're evaluating the model on a more representative portion of the data. This can lead to a more reliable estimate of how well the model will generalize to new, unseen data.\n",
        "\n",
        "Better Detection of Overfitting: A larger validation set can make it easier to detect overfitting. If a model is overfitting, it will have a harder time performing well on a larger and more representative validation set.\n",
        "\n",
        "Reduced Variance in Estimates: With a larger validation set, the estimate of the model's performance is likely to have lower variance. This means that the performance estimate is less sensitive to the particularities of the validation split.\n",
        "\n",
        "Reducing the Percentage of the Validation Set:\n",
        "\n",
        "Increased Variability in Estimates: With a smaller validation set, the estimate of the model's performance can be more variable. It becomes more dependent on the specific data points in the validation set, which may not be fully representative of the overall distribution.\n",
        "\n",
        "Potential Overfitting to Validation Data: If the validation set is very small, the model may start to learn specific features of the validation set rather than generalizing to the broader data distribution. This can lead to overfitting to the validation data.\n",
        "\n",
        "Less Robustness to Data Variability: A smaller validation set is more sensitive to variations in the data. If the validation set happens to have particularly challenging examples, it can give a misleading estimate of the model's true generalization performance.\n",
        "\n",
        "In summary, increasing the percentage of the validation set generally leads to more robust and reliable estimates of model performance, while reducing it can lead to more variable and potentially less reliable estimates. However, it's important to strike a balance based on the available data and the computational resources at hand. A good rule of thumb is to aim for a validation set that's large enough to provide a meaningful evaluation but not so large that it significantly impacts the size of the training set.\n"
      ],
      "metadata": {
        "id": "fc6an7v-WEYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?**\n",
        "The sizes of the training and validation sets play a crucial role in how well the performance on the validation set can predict the performance on the test set. Here's how they affect this prediction:\n",
        "\n",
        "Balanced Sizes:\n",
        "\n",
        "When the sizes of the training and validation sets are balanced and representative of the overall dataset, the performance on the validation set tends to provide a good estimate of how well the model will perform on the test set.\n",
        "Small Validation Set Relative to Training Set:\n",
        "\n",
        "If the validation set is much smaller than the training set, the estimate of model performance on the validation set may be less reliable. This is because the validation set might not adequately capture the variability in the data.\n",
        "Small Training Set Relative to Validation Set:\n",
        "\n",
        "If the training set is much smaller than the validation set, the model may not have enough data to learn the underlying patterns, which can lead to poor generalization even if it performs well on the validation set.\n",
        "Large Validation Set Relative to Training Set:\n",
        "\n",
        "A large validation set relative to the training set can potentially lead to overfitting on the validation set. The model might start to learn specific features of the validation set rather than generalizing to the broader data distribution.\n",
        "Large Training Set Relative to Validation Set:\n",
        "\n",
        "If the training set is much larger than the validation set, the model may perform well on the validation set but fail to generalize to the test set, particularly if the validation set doesn't adequately represent the variability in the data.\n",
        "In summary, having both a reasonably sized training set and a representative validation set is crucial for making accurate predictions about how well the model will perform on the test set. A balanced split of the data helps ensure that the validation set provides a reliable estimate of the model's true generalization performance. It's important to avoid extremes where one set is significantly larger or smaller than the other, as this can lead to biases in the performance estimates.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dx2eo2ycXBj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?**\n",
        "\n",
        "There's no one-size-fits-all answer to the question of what percentage to reserve for the validation set, as the ideal split depends on various factors, including the size of your dataset, the complexity of your model, and the nature of your data.\n",
        "\n",
        "However, a common practice is to use a 70-30 or 80-20 split for training and validation sets, respectively. That is, you allocate 70% or 80% of your data for training and the remaining 30% or 20% for validation.\n",
        "\n",
        "Here's a breakdown of the considerations for each case:\n",
        "\n",
        "70-30 Split:\n",
        "\n",
        "Advantages:\n",
        "More data for training can help the model learn better representations and patterns.\n",
        "A relatively large validation set can provide a good estimate of model performance.\n",
        "Considerations:\n",
        "With a smaller validation set, there's a risk of overfitting to the validation data if the model is too complex.\n",
        "80-20 Split:\n",
        "\n",
        "Advantages:\n",
        "A larger validation set provides a more stable estimate of model performance.\n",
        "With a slightly smaller training set, there may be less risk of overfitting.\n",
        "Considerations:\n",
        "The training set is smaller, so the model might not learn as robustly.\n",
        "Remember, these are general guidelines and the best split may vary based on the specifics of your dataset and problem. Additionally, if your dataset is very small, you might need to allocate a larger portion for training.\n",
        "\n",
        "It's also worth considering techniques like cross-validation, where you repeatedly split the data into different training and validation sets, to get a more robust estimate of model performance. This can be especially useful when the dataset is small or when you want to make sure the performance estimate is not overly sensitive to a particular split.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "44skQPsfXrcL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f554416-794a-4906-dbd4-b9c9e2aed2c8"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Does averaging the validation accuracy across multiple splits give more consistent results?**\n",
        "Reduced Variance: Averaging over multiple splits helps reduce the impact of randomness or variability in the data. If a single split happens to be particularly unrepresentative, it won't dominate the evaluation.\n",
        "\n",
        "Better Generalization: By evaluating the model on multiple different subsets of the data, you get a better estimate of how well it will perform on unseen data in general. This can be especially important when the dataset is small or has some inherent variability.\n",
        "\n",
        "Robustness to Dataset Imbalances: If the data is imbalanced (i.e., one class has significantly more samples than another), different splits may highlight different challenges. Averaging over these splits gives a more comprehensive view of the model's performance.\n",
        "\n",
        "Identifying Overfitting: It can help identify if the model is overfitting. If the model performs significantly better on the training data compared to the validation data across all splits, it's a sign of overfitting.\n",
        "\n",
        "Better Confidence Intervals: When you have multiple validation scores, you can also calculate confidence intervals or statistical tests to quantify the uncertainty around the estimated performance.\n",
        "\n",
        "Parameter Tuning: When doing hyperparameter tuning, cross-validation can be used to assess how different sets of hyperparameters affect model performance in a more robust way.\n",
        "\n",
        "However, it's worth noting that cross-validation can be computationally expensive, especially if the model is complex or the dataset is very large. In some cases, a single train-test split may be sufficient, but cross-validation is generally recommended when it's feasible.\n",
        "\n",
        "The choice of how many splits (e.g., 5-fold, 10-fold, etc.) to use in cross-validation depends on the specific dataset, but 5 or 10-fold cross-validation are common choices. Keep in mind that as the number of folds increases, the computational cost also increases.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1EFKviPTJar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Does it give more accurate estimate of test accuracy?**\n",
        "Cross-validation, while providing a more reliable estimate of a model's performance compared to a single train-test split, doesn't directly give a more accurate estimate of the test accuracy.\n",
        "\n",
        "In a typical machine learning workflow, you use cross-validation on your training data to evaluate the model's performance during development and hyperparameter tuning. The test accuracy is then assessed on a separate, held-out test set that the model has never seen before.\n",
        "\n",
        "The purpose of this separate test set is to give you an unbiased estimate of how well the model is likely to perform on completely new, unseen data. This is crucial for understanding how the model will generalize to real-world, out-of-sample data.\n",
        "\n",
        "Using cross-validation on the training set doesn't replace the need for a separate test set, but it helps you make more informed decisions about the model's performance during development. It provides insights into how well the model is learning patterns from the data, which hyperparameters are working best, and if the model is overfitting.\n",
        "\n",
        "So, in summary, while cross-validation aids in model development and hyperparameter tuning, it doesn't directly replace or enhance the estimate of test accuracy. The held-out test set remains a critical part of the evaluation process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ioEOa73TfPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?**\n",
        "More Stable Estimate: As the number of iterations increases, the estimate of the model's performance tends to become more stable and less sensitive to the particularities of any one random split. This means that with a larger number of iterations, you are likely to get a more reliable estimate of the model's true performance.\n",
        "\n",
        "Increased Computational Cost: However, more iterations come at a computational cost. Each additional fold requires training and evaluating the model multiple times, which can be time-consuming, especially for complex models or large datasets.\n",
        "\n",
        "Reduced Bias, Potentially Increased Variance: With a larger number of iterations, the bias of the estimate tends to decrease. This is because you're using more of the data for both training and validation. However, there's a trade-off, as increasing the number of folds may also lead to an increase in variance. This is because with more folds, each training set becomes smaller, potentially making it harder for the model to learn the underlying patterns.\n",
        "\n",
        "Diminishing Returns: There are diminishing returns associated with increasing the number of iterations. Going from, say, 3-fold to 5-fold cross-validation can make a significant difference. However, going from 10-fold to 15-fold might not provide as much additional benefit.\n",
        "\n",
        "Practical Considerations: In practice, the choice of the number of iterations depends on factors like the size of your dataset, the computational resources available, and how stable you need the estimate to be. Common choices are 5-fold and 10-fold cross-validation.\n",
        "\n",
        "In summary, while increasing the number of iterations generally leads to a more stable estimate of a model's performance, there are practical considerations to take into account. You'll want to strike a balance between computational resources and the level of confidence you need in your performance estimate."
      ],
      "metadata": {
        "id": "IKTsjS7MUr0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?**\n",
        "\n",
        "Increasing the number of iterations in cross-validation can help when dealing with a very small dataset. However, it's important to keep in mind some important considerations:\n",
        "\n",
        "More Stable Estimates: With a very small dataset, there's a higher chance that a single train-test split may not be representative of the overall data distribution. By increasing the number of iterations (e.g., using 10-fold or higher), you're effectively using more of the data for training and validation in each split. This can lead to more stable and reliable performance estimates.\n",
        "\n",
        "Mitigating Overfitting on Small Datasets: Small datasets are more prone to overfitting. Cross-validation allows you to assess the model's performance on multiple subsets of the data, which can help you gauge whether the model is consistently learning meaningful patterns or if it's just fitting noise in the data.\n",
        "\n",
        "Practical Limitations: While increasing the number of iterations can be beneficial, there are practical limitations. Extremely small datasets may not provide enough samples to reliably train and evaluate a model, even with cross-validation. In such cases, collecting more data or considering alternative strategies (e.g., transfer learning, data augmentation) may be necessary.\n",
        "\n",
        "Computational Considerations: With a very small dataset, even a single iteration of cross-validation can be computationally efficient. However, if the dataset is extremely small, the model may struggle to generalize effectively regardless of the number of iterations.\n",
        "\n",
        "Balancing Train and Validation Set Sizes: As you increase the number of folds, the size of the validation set in each fold decreases. This can be problematic if the validation set becomes too small to provide a meaningful evaluation.\n",
        "\n",
        "In summary, increasing the number of iterations in cross-validation can be a useful strategy for dealing with very small datasets. However, there are practical limits, and other approaches like data augmentation or transfer learning may also be considered in cases where data scarcity is a significant challenge.\n"
      ],
      "metadata": {
        "id": "1LITPdc0VGEi"
      }
    }
  ]
}